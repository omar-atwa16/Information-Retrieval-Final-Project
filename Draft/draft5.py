import streamlit as st
import time
from datetime import datetime
import pandas as pd
from Bio import Entrez
import re
from collections import defaultdict
from typing import Dict, Set, List
import pickle
import os
import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer, SnowballStemmer, WordNetLemmatizer

# download nltk resources
nltk.download('stopwords', quiet=True)
nltk.download('wordnet', quiet=True)

# =========================
# ENTrez CONFIG
# =========================
Entrez.email = "omar.atwaa16@gmail.com"

# =========================
# TEXT PREPROCESSOR CLASS (exactly as you sent)
# =========================
class TextPreprocessor:
    def __init__(self):
        self.stop_words = set(stopwords.words('english'))
        self.porter = PorterStemmer()
        self.stemmer = SnowballStemmer('english')
        self.lemmatizer = WordNetLemmatizer()

    def tokenize(self, text):
        return re.findall(r'\b\w+\b', text)

    def normalize(self, tokens):
        return [t.lower() for t in tokens if t.isalpha()]

    def remove_stopwords(self, tokens):
        return [t for t in tokens if t not in self.stop_words]

    def stem(self, tokens, algorithm='porter'):
        if algorithm == 'porter':
            return [self.porter.stem(t) for t in tokens]
        elif algorithm == 'snowball':
            return [self.stemmer.stem(t) for t in tokens]
        return tokens

    def lemmatize(self, tokens):
        return [self.lemmatizer.lemmatize(t) for t in tokens]

    def process_text(self, text):
        tokens = self.tokenize(text)          # fixed typo here
        normalized = self.normalize(tokens)
        filtered = self.remove_stopwords(normalized)
        stemmed = self.stem(filtered)
        lemmatized = self.lemmatize(stemmed)
        return lemmatized

preprocessor = TextPreprocessor()

# =========================
# INVERTED INDEX
# =========================
def build_inverted_index(documents: Dict[str, str]) -> Dict[str, Set[str]]:
    """Build or update an inverted index from a dict of documents"""
    if os.path.exists("inverted_index.pkl"):
        with open("inverted_index.pkl", "rb") as f:
            inverted_index = pickle.load(f)
    else:
        inverted_index = defaultdict(set)

    for doc_id, text in documents.items():
        tokens = preprocessor.process_text(text)
        for token in tokens:
            inverted_index[token].add(doc_id)

    with open("inverted_index.pkl", "wb") as f:
        pickle.dump(inverted_index, f)

    return dict(inverted_index)

# =========================
# FETCH ARTICLES (same as before)
# =========================
def fetch_article_details(pmid: str):
    try:
        handle = Entrez.efetch(db="pubmed", id=pmid, rettype="xml", retmode="xml")
        record = Entrez.read(handle)
        handle.close()
        article = record['PubmedArticle'][0]
        medline = article['MedlineCitation']
        article_data = medline['Article']

        # Abstract
        abstract_text = ""
        if 'Abstract' in article_data and 'AbstractText' in article_data['Abstract']:
            abstract_parts = article_data['Abstract']['AbstractText']
            if isinstance(abstract_parts, list):
                abstract_text = " ".join(str(x) for x in abstract_parts)
            else:
                abstract_text = str(abstract_parts)

        # Authors
        authors_list = []
        for author in article_data.get('AuthorList', []):
            last = author.get('LastName', '')
            initials = author.get('Initials', '')
            full = f"{last} {initials}".strip()
            if full:
                authors_list.append(full)

        # Keywords
        keywordsList = []
        if 'KeywordList' in medline:
            for kw_list in medline['KeywordList']:
                for kw in kw_list:
                    keywordsList.append(str(kw))

        details = {
            'pmid': pmid,
            'title': article_data.get('ArticleTitle', ''),
            'abstract': abstract_text,
            'authors': authors_list,
            'journal': article_data.get('Journal', {}).get('Title', ''),
            'pub_date': article_data.get('Journal', {}).get('JournalIssue', {}).get('PubDate', {}),
            'keywords': keywordsList
        }
        return details
    except Exception as e:
        st.error(f"Error fetching PMID {pmid}: {e}")
        return None

def search_pubmed(query: str, max_results: int = 10, year: str = None):
    if year and year != "All Years":
        query = f"{query} AND {year}[pdat]"
    handle = Entrez.esearch(db="pubmed", term=query, retmax=max_results, sort="relevance")
    record = Entrez.read(handle)
    handle.close()
    return record.get("IdList", [])

def fetch_multiple_articles(pmids, delay: float = 0.4):
    articles = []
    progress_bar = st.progress(0)
    status_text = st.empty()
    for i, pmid in enumerate(pmids):
        status_text.text(f"Fetching article {i + 1}/{len(pmids)}: PMID {pmid}")
        details = fetch_article_details(pmid)
        if details:
            articles.append(details)
        progress_bar.progress((i + 1) / len(pmids))
        time.sleep(delay)
    progress_bar.empty()
    status_text.empty()
    return articles

# ======================
# EVALUATION METRICS
# ======================
def precision_at_k(retrieved, relevant, k):
    return sum(1 for d in retrieved[:k] if d in relevant) / k

def recall_at_k(retrieved, relevant, k):
    return sum(1 for d in retrieved[:k] if d in relevant) / len(relevant)

def f1(p, r):
    return 2 * p * r / (p + r) if p + r else 0

# =========================
# INITIALIZE SESSION STATE
# =========================
if "retrieved_docs" not in st.session_state:
    st.session_state["retrieved_docs"] = []
if "article_cache" not in st.session_state:
    st.session_state["article_cache"] = {}

# =========================
# STREAMLIT APP (same layout as before)
# =========================
st.set_page_config(page_title="PubMed Article Search + Inverted Index", page_icon="ðŸ”¬", layout="wide")
st.title("ðŸ”¬ PubMed Article Search + Inverted Index")
st.markdown("Search and retrieve scientific articles from PubMed and build a persistent inverted index")

tab1, tab2, tab3, tab4 = st.tabs([
    "ðŸ” ID Search",
    "ðŸ“„ Query Search",
    "ðŸ“Š Evaluation",
    "Inverted Index Search"
])

# --- TAB 1 ---
with tab1:
    st.header("Fetch Single Article by PMID")
    pmid_input = st.text_input("Enter PubMed ID (PMID):", placeholder="e.g., 31311655")
    if st.button("Fetch Article", key="fetch_single"):
        if pmid_input:
            with st.spinner("Fetching article..."):
                article = fetch_article_details(pmid_input)
                if article:
                    # Add to cache
                    st.session_state["article_cache"][pmid_input] = article
                    # Update inverted index
                    doc_dict = {pmid_input: article['abstract']}
                    build_inverted_index(doc_dict)
                    
                    st.success("Article retrieved successfully!")
                    st.subheader(article['title'])
                    col1, col2 = st.columns(2)
                    with col1:
                        st.markdown(f"**PMID:** {article['pmid']}")
                        st.markdown(f"**Journal:** {article['journal']}")
                        pub_year = article['pub_date'].get('Year', 'N/A')
                        pub_month = article['pub_date'].get('Month', '')
                        st.markdown(f"**Publication Date:** {pub_month} {pub_year}")
                    with col2:
                        if article['authors']:
                            st.markdown(f"**Authors:** {', '.join(article['authors'][:3])}{'...' if len(article['authors']) > 3 else ''}")
                        if article['keywords']:
                            st.markdown(f"**Keywords:** {', '.join(article['keywords'][:5])}")
                    st.markdown("---")
                    st.subheader("Abstract")
                    st.write(article['abstract'] if article['abstract'] else "No abstract available")
        else:
            st.warning("Please enter a PMID")

# --- TAB 2 ---
with tab2:
    st.header("Search Multiple Articles")
    query_input = st.text_input("Enter search query:", placeholder="e.g., cancer immunotherapy")
    col1, col2 = st.columns(2)
    with col1:
        year_filter = st.selectbox("Filter by Year:", ["All Years"] + [str(year) for year in range(2024, 1999, -1)])
    with col2:
        max_results = st.slider("Number of results:", min_value=1, max_value=100, value=10)

    if st.button("Search Articles", key="search_multiple"):
        if query_input:
            with st.spinner("Searching PubMed..."):
                pmids = search_pubmed(query_input, max_results, year_filter)
                if pmids:
                    # Store retrieved docs for evaluation
                    st.session_state["retrieved_docs"] = pmids
                    
                    st.success(f"Found {len(pmids)} articles. Fetching details...")
                    articles = fetch_multiple_articles(pmids)
                    if articles:
                        # Add to cache
                        for article in articles:
                            st.session_state["article_cache"][article['pmid']] = article
                        
                        df = pd.DataFrame(articles)
                        df['authors_str'] = df['authors'].apply(lambda x: '; '.join(x[:3]) + ('...' if len(x) > 3 else '') if isinstance(x, list) else str(x))
                        df['keywords_str'] = df['keywords'].apply(lambda x: ', '.join(x[:5]) if isinstance(x, list) else str(x))
                        df['year'] = df['pub_date'].apply(lambda x: x.get('Year', 'N/A') if isinstance(x, dict) else 'N/A')
                        st.subheader(f"Results: {len(articles)} articles")
                        # Display articles
                        for idx, article in enumerate(articles):
                            with st.expander(f"{idx + 1}. {article['title']}", expanded=(idx==0)):
                                col1, col2 = st.columns([2,1])
                                with col1:
                                    st.markdown(f"**PMID:** {article['pmid']}")
                                    st.markdown(f"**Journal:** {article['journal']}")
                                    pub_year = article['pub_date'].get('Year', 'N/A')
                                    pub_month = article['pub_date'].get('Month', '')
                                    st.markdown(f"**Date:** {pub_month} {pub_year}")
                                with col2:
                                    if article['keywords']:
                                        st.markdown(f"**Keywords:** {', '.join(article['keywords'][:3])}")
                                if article['authors']:
                                    st.markdown(f"**Authors:** {', '.join(article['authors'][:5])}{'...' if len(article['authors'])>5 else ''}")
                                st.markdown("**Abstract:**")
                                st.write(article['abstract'] if article['abstract'] else "No abstract available")

                        # --- INVERTED INDEX UPDATE ---
                        doc_dict = {art['pmid']: art['abstract'] for art in articles if art['abstract']}
                        inverted_index = build_inverted_index(doc_dict)
                        st.success(f"Inverted index updated! Total tokens: {len(inverted_index)}")

                        st.subheader("Sample tokens in index")
                        for token in list(inverted_index.keys())[:10]:
                            st.write(f"**{token}**: {list(inverted_index[token])[:5]}")

                        # Download CSV
                        st.markdown("---")
                        csv_df = df[['pmid','title','abstract','authors_str','journal','year','keywords_str']].copy()
                        csv_df.columns = ['PMID','Title','Abstract','Authors','Journal','Year','Keywords']
                        csv = csv_df.to_csv(index=False).encode('utf-8')
                        st.download_button(label="ðŸ“¥ Download Results as CSV", data=csv,
                                           file_name=f"pubmed_search_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                                           mime="text/csv")
                else:
                    st.warning("No articles found for your query")

# ======================
# TAB 3 â€” EVALUATION
# ======================
with tab3:
    st.header("Evaluation Metrics")
    
    if not st.session_state["retrieved_docs"]:
        st.warning("No search results to evaluate yet. Run a search in the 'Query Search' tab first.")
    else:
        st.info(f"Evaluating {len(st.session_state['retrieved_docs'])} retrieved documents from last search")
        
        k = st.slider("k value:", 1, len(st.session_state["retrieved_docs"]), 5)
        retrieved = st.session_state["retrieved_docs"]
        
        # For demo purposes, assume all retrieved are relevant
        # In a real scenario, you'd want user input for relevant docs
        relevant = set(retrieved)

        p = precision_at_k(retrieved, relevant, k)
        r = recall_at_k(retrieved, relevant, k)
        f = f1(p, r)

        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("Precision@k", round(p, 3))
        with col2:
            st.metric("Recall@k", round(r, 3))
        with col3:
            st.metric("F1-score", round(f, 3))
        
        st.markdown("---")
        st.subheader("Retrieved Documents")
        for i, pmid in enumerate(retrieved[:10], 1):
            st.text(f"{i}. PMID: {pmid}")
        if len(retrieved) > 10:
            st.text(f"... and {len(retrieved) - 10} more")

# ---------------------------
# TAB 4 â€” FILTER CACHED ARTICLES USING INVERTED INDEX
# ---------------------------
with tab4:
    st.header("Search Cached Articles by Token")
    
    if not st.session_state["article_cache"]:
        st.info("No articles in cache yet. Run a search first in 'ID Search' or 'Query Search' tabs.")
    else:
        st.success(f"Total cached articles: {len(st.session_state['article_cache'])}")
        
        # input token to filter
        search_token = st.text_input("Enter a keyword to filter cached articles:", placeholder="e.g., cancer")
        
        if st.button("Filter Cache"):
            if not search_token:
                st.warning("Please enter a token to filter.")
            else:
                # Process the search token the same way
                processed_tokens = preprocessor.process_text(search_token)
                
                if not processed_tokens:
                    st.warning("Search token was filtered out during preprocessing.")
                else:
                    search_token_processed = processed_tokens[0]
                    
                    # load inverted index
                    if os.path.exists("inverted_index.pkl"):
                        with open("inverted_index.pkl", "rb") as f:
                            inverted_index = pickle.load(f)
                    else:
                        inverted_index = {}

                    # get matching pmids from index
                    matching_pmids = inverted_index.get(search_token_processed, set())

                    # get articles from cache
                    filtered_articles = [
                        art for pmid, art in st.session_state["article_cache"].items()
                        if pmid in matching_pmids
                    ]

                    if filtered_articles:
                        st.success(f"{len(filtered_articles)} article(s) match '{search_token}' (processed as '{search_token_processed}')")
                        for art in filtered_articles:
                            st.subheader(art["title"])
                            st.markdown(f"**PMID:** {art['pmid']}")
                            st.write(art["abstract"])
                            st.divider()
                    else:
                        st.info(f"No cached articles contain the token '{search_token_processed}'")

st.markdown("---")
st.caption("Data retrieved from PubMed using NCBI Entrez API")